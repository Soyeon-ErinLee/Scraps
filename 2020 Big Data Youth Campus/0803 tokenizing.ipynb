{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = kolaw.open(kolaw.fileids()[0]).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "a = defaultdict(lambda:0)\n",
    "for _ in corpus.split():\n",
    "    a[_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국민생활의 1\n",
      "국민투표에 3\n",
      "국민에게 2\n",
      "국민으로부터 1\n",
      "국민이 2\n",
      "국민전체에 1\n",
      "국민에 1\n",
      "국민의 7\n",
      "국민은 35\n",
      "국민을 1\n",
      "국민 2\n",
      "국민경제의 3\n",
      "국민경제자문회의를 1\n",
      "국민투표의 1\n",
      "국민투표사무에 1\n",
      "국민경제상 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in a.items():\n",
    "    if k.startswith('국민'):\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-e7ae484f9121>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-e7ae484f9121>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    count(bigram('국민'))/count('국') -> P1\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# '국민'이 가장 중요한 단어임을 뽑아낼 수 있을까?\n",
    "count(bigram('국민'))/count('국') -> P1\n",
    "count(trigram('국민?'))/count('국민') -> P2\n",
    "P1 > P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = defaultdict(lambda:0)\n",
    "R = defaultdict(lambda:0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(_)-1):\n",
    "    L[_[:i+1]] += 1 # 어근\n",
    "    R[_[i+1:]] += 1 # 조사, 어미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(<function __main__.<lambda>()>, {'행': 1, '행한': 1, '행한다': 1}),\n",
       " defaultdict(<function __main__.<lambda>()>, {'한다.': 1, '다.': 1, '.': 1}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = defaultdict(lambda:0)\n",
    "R = defaultdict(lambda:0)\n",
    "for _ in corpus.split():\n",
    "    if len(_) > 1:\n",
    "        for i in range(1,len(_)-1):\n",
    "            L[_[:i+1]] += 1\n",
    "            R[_[i+1:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('법률', 123),\n",
       " ('국회', 77),\n",
       " ('의하', 72),\n",
       " ('대통', 68),\n",
       " ('대통령', 63),\n",
       " ('국민', 61),\n",
       " ('아니', 58),\n",
       " ('헌법', 57),\n",
       " ('있다', 57),\n",
       " ('한다', 56),\n",
       " ('제1', 54),\n",
       " ('정하', 54),\n",
       " ('국가', 48),\n",
       " ('때에', 43),\n",
       " ('국무', 32),\n",
       " ('필요', 30),\n",
       " ('정한', 28),\n",
       " ('정한다', 28),\n",
       " ('위하', 27),\n",
       " ('②국', 25)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(L.items(), key=lambda _:_[1], reverse=True))[:20] # 어근, 명사 등 중요하다고 생각되는 애들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('의', 415),\n",
       " ('.', 339),\n",
       " ('는', 274),\n",
       " ('에', 230),\n",
       " ('을', 205),\n",
       " ('은', 184),\n",
       " ('다.', 172),\n",
       " ('이', 148),\n",
       " ('여', 145),\n",
       " ('조', 137),\n",
       " ('를', 128),\n",
       " ('로', 113),\n",
       " (',', 101),\n",
       " ('한', 95),\n",
       " ('과', 85),\n",
       " ('한다.', 83),\n",
       " ('할', 74),\n",
       " ('지', 57),\n",
       " ('야', 52),\n",
       " ('으로', 49)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(R.items(), key=lambda _:_[1], reverse=True))[:20] # 어근, 명사 등 중요하다고 생각되는 애들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Kkma, Okt, Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('싹', 'NNG'),\n",
       "  ('쓰리', 'NNG'),\n",
       "  ('의', 'JKG'),\n",
       "  ('다시', 'MAG'),\n",
       "  ('여름', 'NNG'),\n",
       "  ('바닷가', 'NNG')],\n",
       " [('싹', 'Noun'),\n",
       "  ('쓰리의', 'Adjective'),\n",
       "  ('다시', 'Noun'),\n",
       "  ('여름', 'Noun'),\n",
       "  ('바닷가', 'Noun')],\n",
       " [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"싹쓰리의 다시 여름 바닷가\" \n",
    "kkma = Kkma()\n",
    "okt = Okt()\n",
    "hann = Hannanum()\n",
    "kkma.pos(s),okt.pos(s),hann.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "corpus = \"\".join([kobill.open(_).read() for _ in kobill.fileids()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Erin\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = word_tokenize(corpus)\n",
    "L = dict()\n",
    "R = dict()\n",
    "for term in voca:\n",
    "    if len(term)>1:\n",
    "        for i in range(1,len(term)-1):\n",
    "            lkey = term[:i+1]\n",
    "            rkey = term[i+1:]\n",
    "            if lkey in L.keys():\n",
    "                L[lkey] +=1\n",
    "            else:\n",
    "                L[lkey] = 0\n",
    "            if rkey in R.keys():\n",
    "                R[rkey] +=1\n",
    "            else:\n",
    "                R[rkey] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('육아', 147),\n",
       " ('육아휴', 146),\n",
       " ('육아휴직', 125),\n",
       " ('20', 102),\n",
       " ('제1', 86),\n",
       " ('발생', 77),\n",
       " ('201', 70),\n",
       " ('행정', 69),\n",
       " ('이하', 66),\n",
       " ('육아휴직자', 55)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(L.items(), key = lambda _:_[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('의', 380),\n",
       " ('에', 319),\n",
       " ('는', 253),\n",
       " ('을', 243),\n",
       " ('를', 163),\n",
       " ('로', 150),\n",
       " ('한', 138),\n",
       " ('다', 136),\n",
       " ('이', 122),\n",
       " ('과', 121)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(R.items(), key = lambda _:_[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"비투비 이창섭, 말년휴가 나왔다..8월 21일 미 복귀 전역\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation 공백으로 치환\n",
    "from string import punctuation\n",
    "import re\n",
    "s = re.sub('[{}]'.format(re.escape(punctuation)), ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or\n",
    "re.escape(punctuation)\n",
    "# s = re.sub(re.escape(punctuation), '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'비투비 이창섭  말년휴가 나왔다  8월 21일 미 복귀 전역'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('비', 'NNG'),\n",
       "  ('투비', 'NNG'),\n",
       "  ('이', 'MDT'),\n",
       "  ('창섭', 'UN'),\n",
       "  ('말년', 'NNG'),\n",
       "  ('휴가', 'NNG'),\n",
       "  ('나오', 'VV'),\n",
       "  ('았', 'EPT'),\n",
       "  ('다', 'ECS'),\n",
       "  ('8', 'NR'),\n",
       "  ('월', 'NNM'),\n",
       "  ('21', 'NR'),\n",
       "  ('일', 'NNM'),\n",
       "  ('미', 'NNG'),\n",
       "  ('복귀', 'NNG'),\n",
       "  ('전역', 'NNG')],\n",
       " [('비투비', 'Noun'),\n",
       "  ('이창섭', 'Noun'),\n",
       "  ('말년', 'Noun'),\n",
       "  ('휴가', 'Noun'),\n",
       "  ('나왔다', 'Verb'),\n",
       "  ('8월', 'Number'),\n",
       "  ('21일', 'Number'),\n",
       "  ('미', 'Adjective'),\n",
       "  ('복귀', 'Noun'),\n",
       "  ('전역', 'Noun')],\n",
       " [])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(s),okt.pos(s),hann.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비투비\n",
      "{'비': 1.0}\n",
      "투\n",
      "\n",
      "이창섭\n",
      "명사추정\n",
      "\n",
      "말년휴가\n",
      "{'가': 1.0}\n",
      "말년휴\n",
      "\n",
      "나왔다\n",
      "{'다': 1.0}\n",
      "나왔\n",
      "\n",
      "8월\n",
      "명사추정\n",
      "\n",
      "21일\n",
      "{'일': 1.0}\n",
      "21\n",
      "\n",
      "복귀\n",
      "명사추정\n",
      "\n",
      "전역\n",
      "명사추정\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in word_tokenize(s):\n",
    "    if len(term)>1:\n",
    "        print(term)\n",
    "        prob=dict()\n",
    "        for i in range(1,len(term)-1):\n",
    "            rkey = term[i+1:]\n",
    "            if rkey in R.keys():\n",
    "                prob[rkey] = R[rkey]\n",
    "        total = sum(prob.values())\n",
    "        if len(prob.keys()) == 0:\n",
    "            print(\"명사추정\")\n",
    "        else:\n",
    "            maxkey = max(prob, key=prob.get)\n",
    "            print({k:v/total for k, v in prob.items()})\n",
    "            print(re.sub(maxkey,'',term))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아\n",
      "**\n",
      "짜증나\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"띠발\"]\n",
    "s = \"아 띠발 짜증나\"\n",
    "for _ in word_tokenize(s):\n",
    "    if _ in stopwords:\n",
    "        print('*'*len(_))\n",
    "    else:\n",
    "        print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 *** 짜증나\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"띠발\", \"띠발뇬\"]\n",
    "s = \"아 띠발뇬 짜증나\"\n",
    "_s = list()\n",
    "for _ in word_tokenize(s):\n",
    "    if _ in stopwords:\n",
    "        _s.append('*'*len(_))\n",
    "    else:\n",
    "        _s.append(_)\n",
    "print(' '.join(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 ** 발 짜증나 **\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"띠발\", \"띠발뇬\"]\n",
    "s = \"아 띠!@#$#발 ㄸ발 짜증나 띠1발\"\n",
    "s = re.sub('[{}]'.format(re.escape(punctuation)),'',s) # 기호\n",
    "s = re.sub('\\B[0-9]\\B', '', s) # 숫자\n",
    "s = re.sub('[ㄱ-ㅎㅏ-ㅣ]','',s) # 자음, 모음 단독\n",
    "_s = list()\n",
    "for _ in word_tokenize(s):\n",
    "    if re.search(r'({})'.format('|'.join(stopwords)), _):\n",
    "        _s.append('*'*len(_))\n",
    "    else:\n",
    "        _s.append(_)\n",
    "print(' '.join(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n",
    "def convert_data(data): # {'문자열': 빈도}\n",
    "    newdata = dict()\n",
    "    for k, v in data.items():\n",
    "        newdata[' '.join(['<w'] + list(k) + ['</w>'])] = v # 덮어쓰기\n",
    "    return newdata\n",
    "\n",
    "def find_pair(data): # 문자열 -> 쌍으로 변환\n",
    "    pair = defaultdict(lambda:0)\n",
    "    for k,v in data.items(): # l o w </w> : 5\n",
    "        tokens = k.split() # [l, o, w, </w>]\n",
    "        for i in range(len(tokens)-1): # (l,o):5, (o, w):5\n",
    "            pair[tuple(tokens[i:i+2])] += v\n",
    "    return pair\n",
    "\n",
    "import re\n",
    "def merge_pair(data, key): # max pattern -> 합치기\n",
    "    newdata = dict()\n",
    "    for k, v in data.items():\n",
    "        newkey = re.sub(' '.join(key), ''.join(key), k) # 'e s'\n",
    "        newdata[newkey] = v\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('아', 'VV'),\n",
       "  ('아', 'ECS'),\n",
       "  ('띠', 'NNG'),\n",
       "  ('!', 'SF'),\n",
       "  ('@', 'SW'),\n",
       "  ('#', 'SW'),\n",
       "  ('$', 'SW'),\n",
       "  ('#', 'SW'),\n",
       "  ('발', 'NNG'),\n",
       "  ('ㄸ', 'UN'),\n",
       "  ('발', 'NNG'),\n",
       "  ('짜증나', 'VV'),\n",
       "  ('아', 'ECS'),\n",
       "  ('띠', 'NNG'),\n",
       "  ('1', 'NR'),\n",
       "  ('발', 'NNM')],\n",
       " [('아', 'Exclamation'),\n",
       "  ('띠', 'Noun'),\n",
       "  ('!@#', 'Foreign'),\n",
       "  ('$#발', 'Hashtag'),\n",
       "  ('ㄸ', 'KoreanParticle'),\n",
       "  ('발', 'Noun'),\n",
       "  ('짜증나', 'Adjective'),\n",
       "  ('띠', 'Noun'),\n",
       "  ('1', 'Number'),\n",
       "  ('발', 'Noun')],\n",
       " [])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"아 띠!@#$#발 ㄸ발 짜증나 띠1발\"\n",
    "kkma.pos(s), okt.pos(s), hann.pos(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L, R 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_data({'띠발':5, '띠발뇬':2, '띠-발':6, '띠@@!#발':3})\n",
    "\n",
    "for _ in range(3):\n",
    "    pair = find_pair(data)\n",
    "    key = max(pair, key=pair.get)\n",
    "    data = merge_pair(data, key)\n",
    "    \n",
    "temp = list (set([token for _ in data.keys() \n",
    "            for token in _.split() \n",
    "            if len(token) > 1 and token not in ['<w>', '</w>']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'발</w>': 14, '<w띠': 16, '<w띠-': 6})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = defaultdict(lambda:0)\n",
    "for _ in temp:\n",
    "    for k,v in data.items():\n",
    "        if re.search(_, k):\n",
    "            stop[_] += v\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [_ for _ in stop.keys() if re.search('^<w>', _)]\n",
    "R = [_ for _ in stop.keys() if re.search('</w>$', _)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], ['발</w>'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eb819f49d8af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpatterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{0}.+?{1}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "patterns = list()\n",
    "for l in L:\n",
    "    for r in R:\n",
    "        patterns.append(re.compile('{0}.+?{1}'.format(l, r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s = list()\n",
    "for token in s.split():\n",
    "    skip = False\n",
    "    for p in patterns:\n",
    "        if p.search('<w>'+token+'</w>'):\n",
    "            skip = True\n",
    "    if skip:\n",
    "        _s.append('*'*len(token))\n",
    "    else:\n",
    "        _s.append(token)\n",
    "print(' '.join(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cs.virginia.edu/~hw5x/Course/IR2020-Spring/docs/ 자료 받기 Introduction to information ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR Components\n",
    "# 1. Crawler + Indexer -> Crawler / Indexer\n",
    "# 2. Doc Analyzer -> (Improved) BoW\n",
    "# 3. Query -> 2번과 동일 -> DTM(TDM)\n",
    "# 4. Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (Focused) Crawler (BFS) - 네이버 뉴스\n",
    "from requests import get\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from os import listdir\n",
    "\n",
    "urls = ['https://news.naver.com']\n",
    "visited = list()\n",
    "path=''\n",
    "\n",
    "while urls:\n",
    "    seed = urls.pop(0) # Queue\n",
    "    visited.append(seed)\n",
    "    \n",
    "    dom = BeautifulSoup(get(seed).text, 'html.parser')\n",
    "    body = dom.select_one('#articleBodyContents')\n",
    "    \n",
    "    if body: # 뉴스\n",
    "        cid = re.search('rankingSectionId=(\\d+)', seed).group(1)\n",
    "        aid = re.search('aid=(\\d+)', seed).group(1)\n",
    "        file = '{0}-{1}.txt'.format(cid, aid)\n",
    "        with open(path+file, 'w', encoding='utf-8') as f:\n",
    "            f.write(body.text)\n",
    "        body.text\n",
    "    else: # 링크\n",
    "        for news in dom.select('div[id^=ranking_] li > a'):\n",
    "            link = urljoin(seed,a['href'])\n",
    "            if link not in urls and link not in visited:\n",
    "                urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"right.ranking_contents\"></div>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom.select('#right.ranking_contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "정치\n",
      "<h5 class=\"blind\">정치</h5>\n",
      "101\n",
      "경제\n",
      "<h5 class=\"blind\">경제</h5>\n",
      "102\n",
      "사회\n",
      "<h5 class=\"blind\">사회</h5>\n",
      "103\n",
      "생활/문화\n",
      "<h5 class=\"blind\">생활/문화</h5>\n",
      "104\n",
      "세계\n",
      "<h5 class=\"blind\">세계</h5>\n",
      "105\n",
      "IT/과학\n",
      "<h5 class=\"blind\">IT/과학</h5>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for news in dom.select('div[id^=ranking_]'):\n",
    "    print(re.search('\\d{3}', news['id']).group()) # category number\n",
    "    print(news.find().text)\n",
    "    print(news.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"nclicks(rig.rankpol)\" href=\"/main/ranking/read.nhn?mid=etc&amp;sid1=111&amp;rankingType=popular_day&amp;oid=005&amp;aid=0001348190&amp;date=20200803&amp;type=1&amp;rankingSeq=1&amp;rankingSectionId=100\" title=\"“조국은 파렴치한” 비장해진 국대떡볶이 대표 페북 상황\">“조국은 파렴치한” 비장해진 국대떡볶이 대표 페북 상황</a>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom.select('div[id^=ranking_]')[0].select('li > a')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "정치\n",
      "“조국은 파렴치한” 비장해진 국대떡볶이 대표 페북 상황\n",
      "0001348190\n",
      "[단독] 월세 몸소 실천한다던 윤준병, 알고보니 정읍 지역구\n",
      "0003551166\n",
      "\"오거돈·박원순 성범죄 맞나\" 묻자… 끝까지 입닫은 여가부 장관\n",
      "0003551227\n",
      "윤준병 “정읍에서 월세 산다…큰 금액 내고 있지는 않아”\n",
      "0001348185\n",
      "[속보] ‘월세예찬’ 논란 윤준병, 선거법 위반 혐의 기소\n",
      "0003490198\n",
      "\"오거돈·박원순, 권력형 성범죄 맞냐\" 물음에 끝까지 답 못한 여가부 장관\n",
      "0000521888\n",
      "잠자던 '야성' 깨운 윤희숙…통합당 \"합리적 '대안 야당' 되자\"\n",
      "0004391547\n",
      "\"부동산 폭등은 MB·박근혜 정부 때문\"…오늘도 '남탓'한 與\n",
      "0004391479\n",
      "[레이더P] 바뀐 서울 민심…정당지지율 서울서 통합당이 민주당 앞서\n",
      "0004627658\n",
      "[1보] 외교부 \"성추행 의혹 외교관 오늘부로 즉각 귀임 발령\"\n",
      "0011788427\n",
      "101\n",
      "경제\n",
      "미국인이 42채 '싹쓸이' 中유학생은 8채 사서 임대... 만만했던 한국 부동산\n",
      "0000521902\n",
      "갭투자로 아파트 42채 쓸어 담은 미국인, 세무조사 착수\n",
      "0003551211\n",
      "일회용마스크 4종 피부염 유발 우려… 리콜 권고\n",
      "0003024464\n",
      "2년만에 아파트 42채 갭투자 미국인…외국인 다주택자 세무조사\n",
      "0011787910\n",
      "미국인이 갭투자로 42채 구입…외국인도 아파트 쓸어담았다\n",
      "0003022999\n",
      "미국 \"일본의 수출규제는 WTO 심리 대상 아니다\"…일본 편들어\n",
      "0011787924\n",
      "[단독] 文 정부 3년 집값 급등 상위 지역 보니…충청권 '싹쓸이'\n",
      "0004391534\n",
      "부모가 집 있으면 미혼 20대에 취득세 폭탄?… \"중위소득 40% 이상은 독립가구 인정\"\n",
      "0004729825\n",
      "김현아 “임대차법 최대 수혜자, 강남 10억 고액 전세자”\n",
      "0003112561\n",
      "‘갭투자’로 42채…외국인 집주인들 세무조사 착수\n",
      "0010879130\n",
      "102\n",
      "사회\n",
      "천안 ‘집중호우경보’ 발령…‘침수 우려’ 성정지하차도 통제\n",
      "0003301200\n",
      "친구 살해 뒤 무의도에 버렸다…'가방 시신' 사건 20대들 자수\n",
      "0003023007\n",
      "[속보] 강원 홍천군 캠핑장 확진자, 강남 커피숍 머물러...방역당국 \"관련성 확인 중\"\n",
      "0000521914\n",
      "[속보] 가평 펜션에 토사 밀려 들어와… 어린이 등 3명 실종\n",
      "0000521905\n",
      "\"승객 방귀에 이성 잃었다\" 흉기 10여차례 휘두른 택시기사\n",
      "0003022980\n",
      "가평서 토사에 펜션 매몰돼 구조작업중…\"3명 대피 못 해\"\n",
      "0011788202\n",
      "[속보] 서울 집중호우로 올림픽대로 염창IC~동작대교 교통통제\n",
      "0003024492\n",
      "친구 살해 후 인천 무의도에 시신 유기 20대 2명 체포\n",
      "0000521892\n",
      "평택 공장에 토사 덮쳐 3명 사망·1명 중상(종합)\n",
      "0011788086\n",
      "[단독] '한 방' 못찾은 채널A 수사팀, 기소 전날 또 노트북 포렌식\n",
      "0003551217\n",
      "103\n",
      "생활문화\n",
      "'IQ 164' 3살 영재의 놀라운 성장…비결은 부모다\n",
      "0000832053\n",
      "\"오히려 기분 나빠\"…영재 망치는 '독'되는 칭찬\n",
      "0000832054\n",
      "`장대비 내려도 나는 달린다`…위험천만 러닝族 \"뿌듯하다\"\n",
      "0004627628\n",
      "진로 바뀐 4호 태풍 '하구핏'...우리나라로 올 가능성은?\n",
      "0001472759\n",
      "“병 생긴 뒤 후회한다” 당장 토마토가 필요한 사람들 5\n",
      "0000045890\n",
      "폭우로 침수된 KTX천안아산역 인근 지하도\n",
      "0011788124\n",
      "[뉴스피처] \"남자 만날 수 있다\" 영상 올렸다가…감옥 가게 된 여대생\n",
      "0011786953\n",
      "[더오래]모기가 얼마나 싫었으면…정약용이 남긴 시 ‘증문’\n",
      "0003023010\n",
      "'돌밥돌밥' 이제 못하겠어요... 밥 프리를 선언합니다\n",
      "0002279185\n",
      "그때 그 감성 그대로…'추억 소환' 뉴트로 게임 열풍｜아침& 라이프\n",
      "0000244183\n",
      "104\n",
      "세계\n",
      "34살 세계 최연소 핀란드 총리, 코로나 꺾이자 드디어 결혼\n",
      "0003551212\n",
      "틱톡 퇴출 이갈더니 MS 인수 사실상 승인…트럼프 돌변, 왜\n",
      "0003023003\n",
      "美 곳곳으로 배달된 중국발 '의문의 씨앗' 정체 밝혀져\n",
      "0001472887\n",
      "태풍 하구핏, 중국 향한다…샨샤댐 수위 조절에 초긴장\n",
      "0000890782\n",
      "“피라미드 외계인이 만들었다”는 머스크에게 이집트가 통 큰 초청장을 날렸다\n",
      "0002507337\n",
      "교도관 쓰러지자 60명이 일제히 철문 두드려…美 수감자들 화제\n",
      "0003490185\n",
      "英 해변서 4.5m 괴생명체 사체 발견…“고래로 추정”\n",
      "0003112589\n",
      "한국인은 돈 내고 물 마셔라?…日 유명 초밥집의 혐한\n",
      "0003390684\n",
      "212년 된 조각상 발가락이 똑, 셀카 찍고 도망간 남성\n",
      "0003551206\n",
      "美, WTO 회의장서 \"수출규제는 日 안보조치\" 발언 파문\n",
      "0003023022\n",
      "105\n",
      "IT과학\n",
      "“갤노트20 대신 싼 노트10 사자!”…‘반짝’ 특수 [IT선빵!]\n",
      "0001707231\n",
      "27만명→3만명… 네이버통장 ‘한달천하’ [IT선빵!]\n",
      "0001707186\n",
      "'괴짜 천재' 일론 머스크가 막 올린 민간 우주여행 시대\n",
      "0004449443\n",
      "‘배터리 전쟁’ 韓 잰걸음 vs 日·中 뒷걸음…승자는 LG화학\n",
      "0002089610\n",
      "“갤노트20 대신 싼 갤노트10 사자!”…반짝 특수 [IT선빵!]\n",
      "0001707363\n",
      "美 첫 민간 유인우주선 지구 귀환, 우주에서 봤더니…\n",
      "0002195462\n",
      "일론 머스크도 감탄…미국 우주인, 45년 만에 해상 귀환\n",
      "0000890611\n",
      "“누가 콜록했는지 다 압니다”…‘기침 인식 카메라’ 국내 개발\n",
      "0002507356\n",
      "보령제약, 소세포폐암 신약 ‘러비넥테딘’ 희귀의약품 지정\n",
      "0000564412\n",
      "“반갑다 중력아!”…美 첫 유인우주선 ‘크루드래건’ 귀환 성공[청계천 옆 사진관]\n",
      "0003301199\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "pattern1 = re.compile('[{}]'.format(re.escape(punctuation)))\n",
    "for news in dom.select('div[id^=ranking_]'):\n",
    "    print(re.search('\\d{3}',news['id']).group())\n",
    "    print(pattern1.sub('',news.find().text))\n",
    "    for a in news.select('li > a'):\n",
    "        print(a.text.strip())\n",
    "        link = urljoin(seed,a['href'])\n",
    "        print(re.search('aid=(\\d+)',link).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=020&aid=0003301199&date=20200803&type=1&rankingSeq=10&rankingSectionId=105'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation)))\n",
    "pattern2 = re.compile(r'\\b\\w+@(?:[.]?\\w+)+\\b')\n",
    "pattern3 = re.compile(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexer\n",
    "from os import listdir\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileids(path = 'project/'):\n",
    "    return [path+_ for _ in listdir(path)\n",
    "            if re.search('[.]txt$', _)]\n",
    "\n",
    "def cleaning(doc):\n",
    "    return pattern3.sub(' ',\n",
    "            pattern2.sub(' ',\n",
    "            pattern1.sub(' ', f.read()))).strip()\n",
    "\n",
    "def tokenizer1(doc): # 어절\n",
    "    return doc.split()\n",
    "\n",
    "def tokenizer2(tokens, n=2): # 어절 Ngram\n",
    "    ngram = list()\n",
    "    for i in range(len(tokens) - (n-1)):\n",
    "        ngram.append(tokens[i:i+n])\n",
    "    return ngram\n",
    "\n",
    "def tokenizer3(doc, n=2): # 음절 Ngram\n",
    "    ngram = list()\n",
    "    for i in range(len(doc) - (n-1)):\n",
    "        ngram.append(doc[i:i+n])\n",
    "    return ngram\n",
    "\n",
    "def tokenizer4(doc): # 형태소\n",
    "    return [_ for _ in okt.morphs(doc) if 1 < len(_) < 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xec in position 0: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-7b3a52cf06de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'project/105-0003301199.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xec in position 0: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "with open('project/105-0003301199.txt', 'rt', encoding = \"UTF-8\") as f:\n",
    "    doc = cleaning(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n조급해하지말기 \\n진로 명확하게 정하기\\n데이터분석  데이터 analysis\\n2학기 학회 지원 KU BIG 8월 말\\nkaggle  github   개인 프로젝트 진행\\n통계대학원 24학점  6 9 6 3\\n\\n    지금 해야할 것    \\n\\ngithub kaggle DACON\\n\\n토익 토스 오픽 오픽이 토스보단 쉬은듯하다 1 2주 빡세게 해야할 듯\\nMorgan Stanley  모건스탠리 \\n2021 Asia Internship Programs\\n\\n\\n   공모전   \\n서울시 빅데이터캠퍼스 공모전 안내\\n한국거래소 증권 파생상품 경시대회\\n롯데 빅데이터 10월 말 \\n빅콘테스트 7월  \\n\\n삼성SDS Brightics Academy 공모전 6월 https   contest brightics ai  \\n\\n2020 금융 빅데이터 아이디어 공모전  금융보안원 \\n\\n카카오 아레나\\n\\n   자격증   \\nsql  \\n사조사  \\nADsP\\n컴활\\n한국사\\n\\n   어학        \\n토익\\n토스 오픽\\n\\n\\n\\n  미래에 하고싶은 인턴   \\n네이버웹툰 데이터분석 인턴\\n삼성카드 빅데이터 직무 인턴 5월\\n공공빅데이터 청년 인턴십 5월 초 https   bigdata recruiter co kr appsite company index\\n\\n예금보험공사 체험형인턴 5 6월  근데 봉사시간 중요하대\\n한국은행 통계조사보조원\\nibk 기업은행 청년인턴 5월 11월\\n하나은행 대학생인턴 5월\\n\\n산업은행 청년인턴 2 3월  4 8  6 7월  8 12  11 12월  1 4 \\n신용보증기금 청년인턴 2월 7월  3개월 인턴   3 6  8 11 \\n한국투자공사 체험형인턴 7월  9 12 3개월 \\n국민건강보험공단 2월 8월  5개월 \\n\\nP G 인턴\\nsk하이닉스 청년 HY Five 인턴십  4 27      8월부터 11월\\njp morgan intern\\nSC제일은행 2021 인턴십 찾아볼 것 2020 03 26    2021 01 08 졸업 예정자만 지원 가능\\n\\n    가고싶은 직장    \\n삼성카드  경개센 2019 삼성카드  데이터분석직무 \\n신한카드 데이터사이언스 직무\\n국민카드 개개인의 소비 이력과 패턴을 가장 많이 보유한 곳이 카드사\\n헌대카드 데이터분석\\n카카오뱅크 리스크관리\\nSK C C 데이터 분석 직무\\n삼성전자 평가 및 분석\\nLG CNS 빅데이터\\nLG 상사 빅데이터\\n\\n금공 통계직렬\\n금융감독원 4명\\n한국은행 3 4명  법학 통계학 컴공 합해서 13명 이내 \\n한국거래소 3 4명  수학 통계 합해서 한자릿수 이내 \\n예금보험공사 3명  금융통계  \\n서울보증보험 3 5명  경영 경제 법 통계 전산 통틀어서 00명 \\n산업은행 3명  디지털 빅데이터 분야  NCS 20 정보통신 01 정보기술 \\n한국투자공사 리스크관리 2명  금융 재무 투자   통계 \\n\\nNICE평가정보\\n국가정보원\\n한국신용평가\\n카카오\\n\\n\\n    봉사활동    \\n삼성카드 골든벨 스쿨 대학생 강사 대학생만 가능할듯 \\n제8기 FSS 대학생 금융교육봉사단 7월   FSS 금융아카데미\\n통계청 통계교육원 통계교육 재능기부단 이건 내년 3월\\nKRX 금융인성교육 대학생봉사단 해피누리 6 10 \\nYAHO 이것도 12월\\nbnp paribas   ja korea 경제 금융교육 프로그램 10월 말\\n그래도 JA korea 홈페이지 자주 찾아보기\\n\\n\\n\\n\\n다양한 소스로 부터 발생해 각각의 DB에 저장된 데이터를 하나의 큰 데이터 스토리지에 모을 수 있는 데이터 파이프라인 구축\\n\\n지능정보시스템 \\n주로 장기 계획 목적을 위해 외부 정보를 수집  처리하도록 고안된 정보 시스템\\n\\n오픈소스  각종 소프트웨어들의 소스코드를 오픈해 누구나 해당프로그램을 자유롭게 편집 튜닝할수있고 자신이 튜닝하거나 편집한사항을 다른사람들에게 공유할 수도있는 프리저작권의 프로그램들입니다 \\n\\n카드 데이터 분석 인턴 네이버 쳐보기'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pattern1.sub(' ', doc)\n",
    "doc = pattern2.sub(' ',doc)\n",
    "doc = pattern3.sub(' ',doc)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
